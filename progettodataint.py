# -*- coding: utf-8 -*-
"""ProgettoDataInt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FqdrCu_Vv19GOGYblszn96urFKA0YhXw

# Stimare se un account Instagram è fake o reale
- Frattarola Marco, Mastrilli Alice
- Laurea in Ingegneria e Scienze Informatiche
- Programmazione di Data Intensive
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
np.seterr(divide='ignore', invalid='ignore')
import warnings
warnings.filterwarnings("ignore")

"""## Descrizione del problema
L'obiettivo del progetto è quello di predire, tramite una variabile discreta binaria, se un account Instagram è reale o fake.
Sono utilizzati due dataset distinti che hanno features diverse degli account. 
Il dataset in esame contiene informazioni su account Instagram reali o fittizi di ? utenti. 
"""

from google.colab import files
uploaded = files.upload()

real_account = pd.read_json("realAccountData.json")

real_account.tail()

fake_account= pd.read_json("fakeAccountData.json")
fake_account.tail()

"""Il primo dataset si compone di 994 account reali e 200 account fake."""

dataset1 = real_account.append(fake_account, ignore_index=True)

dataset1

test = pd.read_csv("test.csv")
train = pd.read_csv("train.csv")

dataset2 = test.append(train, ignore_index=True)

"""Il secondo dataset ha meno istanze (in totale 696) ma ha più features. """

dataset2

"""# PARTE 1

## Significato delle feature - Dataset1
"""

dataset1.head()

"""-  **userFollowerCount**: numero di seguaci dell'account
-  **userFollowingCount**: numero di seguiti dell'account
-  **userBiographyLength**: numero di caratteri della biografia dell'account
-  **userMediaCount**: numero di post dell'utente
-  **userHasProfilPic**: indica se l'utente ha una foto profilo (1=foto profilo presente, 0=foto profilo assente)
-  **userIsPrivate**: indica se l'utente ha il profilo privato (1=profilo privato, 0=profilo pubblico)
-  **usernameDigitCount**: numero di cifre nell'username
-  **usernameLength**: numero di caratteri dell'username
-  isFake: indica se l'account è realmente esistente o fake (1=account fake, 0=account reale)

La variabile target è **isFake** , binaria, che assume valore 1 se l'account è falso, altrimenti 0.

Per ottenere informazioni sulle features ( media, deviazione standard, distribuzione in termini di masssimi, minimi e percentili), si utilizza il metodo *describe*.
"""

dataset1.describe()

"""Si evince che, nel dataset, in media un account ha più seguiti che seguaci, più della metà degli account sono privati e circa il 17% degli account sono fake, questo potrebbe creare problemi nelle successive fasi di valutazione e modellazione. Visualizziamo, in un grafico a torta, la percentuale di account fake(parte arancione) e reali(parte blu)."""

dataset1['isFake'].value_counts().plot.pie(autopct="%1.1f%%")

"""## Data exploration - Dataset1

Si visualizzano le distribuzioni delle variabili continue del dataset.
"""

plt.figure(figsize=(20,10))

plt.subplot(2,3,1)
plt.title("userFollowerCount")
plt.hist(dataset1["userFollowerCount"], color="yellow")
plt.ylabel("count")
plt.xlabel("follower")

plt.subplot(2,3,2)
plt.title("userFollowingCount")
plt.hist(dataset1["userFollowingCount"], color="orange")
plt.ylabel("count")
plt.xlabel("following")

plt.subplot(2,3,3)
plt.title("userBiographyLength")
plt.hist(dataset1["userBiographyLength"], color="red")
plt.ylabel("count")
plt.xlabel("characters")

plt.subplot(2,3,4)
plt.title("userMediaCount")
plt.hist(dataset1["userMediaCount"], color="purple")
plt.ylabel("count")
plt.xlabel("media")

plt.subplot(2,3,5)
plt.title("usernameDigitCount")
plt.hist(dataset1["usernameDigitCount"], color="green")
plt.ylabel("count")
plt.xlabel("digit")

plt.subplot(2,3,6)
plt.title("usernameLength")
plt.hist(dataset1["usernameLength"], color="blue")
plt.ylabel("count")
plt.xlabel("characters")

"""Dall'istogramma si osserva che la maggior parte degli account ha un numero di follower e following minore di 1000, un numero di post minore di 100 e la lunghezza dell'username compresa tra 10 e 15. Si rappresentano ora i dati con potblox.  """

plt.figure(figsize=(20,10))

plt.subplot(2,3,1)
plt.title("userFollowerCount")
plt.boxplot(dataset1["userFollowerCount"])
plt.ylabel("count")

plt.subplot(2,3,2)
plt.title("userFollowingCount")
plt.boxplot(dataset1["userFollowingCount"])
plt.ylabel("count")

plt.subplot(2,3,3)
plt.title("userBiographyLength")
plt.boxplot(dataset1["userBiographyLength"])
plt.ylabel("characters")

plt.subplot(2,3,4)
plt.title("userMediaCount")
plt.boxplot(dataset1["userMediaCount"])
plt.ylabel("post")

plt.subplot(2,3,5)
plt.title("usernameDigitCount")
plt.boxplot(dataset1["usernameDigitCount"])
plt.ylabel("digit")

plt.subplot(2,3,6)
plt.title("usernameLength")
plt.boxplot(dataset1["usernameLength"])
plt.ylabel("characters")

"""## Esplorazione relazioni tra features - Dataset1

Per esperienza, si può dire che, a condizionare se un profilo è fake o meno sono principalmente i parametri: numero di seguiti/seguaci e numero di post. Vediamo in che modo, al variare di queste features, sono distribuiti i profili fake e reali.

Partiamo da *userFollowingCount*.
"""

dataset1.pivot(columns="isFake")["userFollowingCount"].plot.hist()

"""Dal grafico si evince che gli account fake (quelli in arancione) hanno un numero più alto di seguiti (in blu): questa cosa è alquanto intuitiva, dal momento che vengono solitamente creati per "seguire" chi compra followers. Si provi a vedere il numero medio di seguiti di un account fake e non."""

dataset1[dataset1["isFake"]==0]["userFollowingCount"].mean()

dataset1[dataset1["isFake"]==1]["userFollowingCount"].mean()

"""I risultati confermamo quanto sospettato: si vede che la media dei seguiti dei profili fake è più del doppio dei reali.

Osserviamo ora *userFollowerCount*: quello che ci si aspetta è che il numero di seguaci degli account reali è maggiore di quelli fake.
"""

dataset1.pivot(columns="isFake")["userFollowerCount"].plot.hist()

dataset1[dataset1["isFake"]==0]["userFollowerCount"].mean()

dataset1[dataset1["isFake"]==1]["userFollowerCount"].mean()

"""La previsione era esatta: gli account reali sono seguiti quasi 4 volte in più dei fake. Dunque, appare evidente che le variabili *userFollowingCount* e *userFollowerCount* sono rilevanti ai fini della modellazione, come si vedrà nei passi successivi
.

Nel seguente dizionario viene associato un colore ad ogni classe. **Gli account falsi verranno rappresentati dal colore rosso, quelli veri dal blu.**
"""

account_color_map ={0:"blue", 1:"red"}
account_colors= dataset1["isFake"].map(account_color_map)

dataset1.plot.scatter("userFollowerCount", "userFollowingCount", c=account_colors, figsize=(8,6))

"""Dal grafico si evince che gli account falsi, concentrati a sinistra, hanno pochi seguiti e molti seguaci, al contrario di quelli veri.

Si prova adesso ad osservare un'altra variabile *userHasProfilPic*: ci si aspetta che la maggior parte di account che non hanno la foto sono fake, quelli che la hanno sono reali. Visualizziamo la distribuzione di questa feature nel grafico.
"""

dataset1.pivot(columns="isFake")["userHasProfilPic"].plot.hist()

"""Come sospettato, tutti gli account reali hanno una foto profilo, mentre, tra i fake, solo una metà la ha.

## Signficato delle feature - Dataset2
"""

dataset2.head()

"""- **profile pic**: Indica se l'utente ha una foto profilo (1 = presente, 0 = assente)
- **nums/length username**: Il rapporto tra i numeri presenti nell username e la lunghezza totale dell'username
- **fullname words**: Il numero di parole presenti nel nome completo.
- **nums/length fullname**: Il rapporto tra i numberi presenti nel nome e la lunghezza totale del nome.
- **name == username**: Indica se il nome è identico allo username ( 1 = identico, 0 = diversi)
- **description length**: La lunghezza della descrizione presente nell'account
- **external URL**: Indica se è presente un URL esterno nella descrizione ( 1 = presente, 0 = assente)
- **private**: Indica se l' utente ha un profilo privato.
- **#posts**: Il numero di post
- **#followers**: Il numero di seguaci
- **#follows**: Il numero di profili seguiti
- **fake**: Indica se il profilio è fake ( 1 = fake, 0 = non fake)

In questo dataset, la variabile target è **fake** , binaria, che assume valore 1 se l'account è falso, altrimenti 0.

Per ottenere informazioni sulle features ( media, deviazione standard, distribuzione in termini di masssimi, minimi e percentili), si utilizza il metodo describe.
"""

dataset2.describe()

"""Dallo studio del dataset, si evince che:
- In media un account ha più seguiti che seguaci.
- Meno della metà degli account sono privati.
- Il numero di account fake e di account reali è lo stesso.
- Il numero di account che hanno il nome uguale all'username è molto basso.

Osserviamo già una sostanziale differenza dal dataset1: in questo secondo set, il numero medio di seguiti è molto più alto dei follows misurati nel dataset1. E' molto probabile, dunque, che per la realizzazione di questo secondo dataset siano stati considerati account di persone molto famose, come profili reali: ciò spiega l'alto numero di follower.

Visualizziamo, in un grafico a torta, la percentuale di account fake (in arancione) e reali (in blu).
"""

dataset2['fake'].value_counts().plot.pie(autopct="%1.1f%%")

"""##Data exploration- dataset2

Si visualizzano le distribuzioni delle variabili continue del dataset
"""

plt.figure(figsize=(30,30))

plt.subplot(4,2,1)
plt.title("numbers/ (length username)")
plt.hist(dataset2["nums/length username"], color="yellow")
plt.ylabel("count")
plt.xlabel("ratio")

plt.subplot(4,2,2)
plt.title("fullname words")
plt.hist(dataset2["fullname words"], color="orange")
plt.ylabel("count")
plt.xlabel("words")

plt.subplot(4,2,3)
plt.title("numbers/(length name)")
plt.hist(dataset2["nums/length fullname"], color="red")
plt.ylabel("count")
plt.xlabel("ratio")

plt.subplot(4,2,4)
plt.title("Description length")
plt.hist(dataset2["description length"], color="purple")
plt.ylabel("count")
plt.xlabel("Description length")

plt.subplot(4,2,5)
plt.title("Number of posts")
plt.hist(dataset2["#posts"], color="green")
plt.ylabel("count")
plt.xlabel("Number of posts")

plt.subplot(4,2,6)
plt.title("Number of followers")
plt.hist(dataset2["#followers"], color="blue")
plt.ylabel("count")
plt.xlabel("Number of followers")

plt.subplot(4,2,7)
plt.title("Number of follows")
plt.hist(dataset2["#follows"], color="grey")
plt.ylabel("count")
plt.xlabel("Number of follows")

"""Dal grafico si puo osservare che ci sono più numeri nell'username che nel nome completo. Inoltre, la maggior parte degli account ha nome e descrizione composti da poche parole. Passiamo adesso ai boxplot."""

plt.figure(figsize=(20,20))

plt.subplot(4,2,1)
plt.title("numbers/ (length username)")
plt.boxplot(dataset2["nums/length username"])
plt.ylabel("ratio")

plt.subplot(4,2,2)
plt.title("fullname words")
plt.boxplot(dataset2["fullname words"])
plt.ylabel("words")

plt.subplot(4,2,3)
plt.title("numbers/(length name)")
plt.boxplot(dataset2["nums/length fullname"])
plt.ylabel("ratio")

plt.subplot(4,2,4)
plt.title("Description length")
plt.boxplot(dataset2["description length"])
plt.ylabel("Description length")

plt.subplot(4,2,5)
plt.title("Number of posts")
plt.boxplot(dataset2["#posts"])
plt.ylabel("Number of posts")

plt.subplot(4,2,6)
plt.title("Number of followers")
plt.boxplot(dataset2["#followers"])
plt.ylabel("Number of followers")

plt.subplot(4,2,7)
plt.title("Number of follows")
plt.boxplot(dataset2["#follows"])
plt.ylabel("Number of follows")

"""##Esplorazione relazioni tra features - Dataset2

Analizziamo alcune variabili e le loro relazioni con la variabile **fake**

**Profile pic**: osserviamo se la presenza di una foto profilo è più riccorrente negli account veri rispetto a quelli falsi
"""

fakeWithProfile = dataset2[(dataset2['fake'] == 1) & (dataset2['profile pic'] == 1)].shape[0]
fakeNoProfile = dataset2[(dataset2['fake'] == 1) & (dataset2['profile pic'] == 0)].shape[0]
realWithProfile = dataset2[(dataset2['fake'] == 0) & (dataset2['profile pic'] == 1)].shape[0]
realNoProfile = dataset2[(dataset2['fake'] == 0) & (dataset2['profile pic'] == 0)].shape[0]

df = pd.DataFrame({'with profile pic':[fakeWithProfile, realWithProfile],
                   'without profile pic':[fakeNoProfile, realNoProfile]}, 
                  index=['Fake accounts','Real accounts'])
df.T.plot.pie(subplots=True, figsize=(10, 30))

"""Possiamo osservare che quasi tutti gli account veri hanno una foto profilo.
Negli account falsi la differenza non è cosi evidente. Si può dedurre che un account senza foto profilo all'interno di questo dataset quasi sicuramente sarà un account falso.
"""

print("Fake with no profile:" , fakeNoProfile)
print("Real with no profile:" , realNoProfile)
print("Fake probability:" , (fakeNoProfile / (fakeNoProfile + realNoProfile)) * 100, "%")

"""Possiamo notare che un account senza foto profilo all'interno del dataset ha una probabilità dell 99% di essere fake.

Si studia ora la feature **num / length username e fullname**. Ci si aspetta che, negli account falsi, i numeri nell'username/fullname sono maggiori rispetto agli account reali, essendo di solito generati randomicamente.

Etichettiamo in **blu gli account veri** e in **rosso quelli falsi**
"""

account_color_map ={0:"blue", 1:"red"}
account_colors= dataset2["fake"].map(account_color_map)

dataset2.plot.scatter("nums/length username", "nums/length fullname", c=account_colors, figsize=(10,10))

"""Dal grafico osserviamo, come previsto, che la maggior parte degli account che hanno più numeri nel nome e nello username sono falsi. Si nota inoltre che la presenza di numeri nello username è molto più ricorrente rispetto al nome completo. Questo comporta una scarsa importanza dell'attributo *nums/length fullname* al fine di distunguere account veri e falsi.

**Follows and followers number**: Confrontiamo la correlazione tra numero di follower e di follows tra account reali e account falsi. Ci aspettiamo che quelli falsi abbiano un numero di seguiti maggiore rispetto ai seguaci.
"""

tmp = dataset2[dataset2["#followers"] < 15000]

tmp.plot.scatter("#followers", "#follows", c=tmp["fake"].map(account_color_map), figsize=(10,10))

"""Come previsto, gli account falsi hanno un un numero maggiore di follows e pochi followers. Negli account veri si nota il contrario, tranne per alcune eccezioni dove  il numero di followers é molto elevato (probabilmente si tratta di account molto famosi). Queste eccezioni sono state escluse dal grafico per facilitarne la visualizzazione.

**#posts**: Analizziamo il numero di post effettuati dagli account. Ci aspettiamo che gli account falsi abbiano effettuatuo un numero basso di post rispetto agli account veri.
"""

dataset2.pivot(columns='fake')["#posts"].sum().plot.pie(autopct="%1.1f%%",subplots='true',
            figsize=(8,8))

"""In effetti, considerando che nel dataset sono presenti lo stesso numeri di account veri e falsi, notiamo che soltanto il 4% dei post totali sono stati effettuati da account falsi."""

fakeMean = dataset2[dataset2["fake"] == True]["#posts"].mean()
realMean = dataset2[dataset2["fake"] == False]["#posts"].mean()


print("Media post account falsi",fakeMean)
print("Media post account veri",realMean)

"""Notiamo anche come in media un account falso effettui circa 8 post, mentre un account vero circa 198

# PARTE 2
"""

from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Perceptron
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import seaborn as sb
from sklearn.metrics import precision_score, recall_score, f1_score

"""Per osservare le variabili più rilevanti, dopo la divisione dei dati in input in training e validation set, si applica una regolarizzazione L1. Si sceglie, come modello, il Perceptron.

## Studio dataset1

Il set y rappresenta l'output, ovvero la sola feature *isFake* , nel set x ci sono tutte le altre feature
"""

y = dataset1["isFake"]
x = dataset1.drop(["isFake"], axis=1)

X_pre_train, X_pre_val, y_pre_train, y_pre_val = train_test_split(
    x,
    y,
    test_size=1/3, random_state=42
)

"""Definiamo i metodi per fare il training del modello e calcolare lo score."""

def get_coefficients(model, index, model_name="model"):
    return pd.Series(model.named_steps[model_name].coef_[0], index=index)
    
def fit(model, X_train, y_train, X_val, y_val):
    model.fit(X_train, y_train)
    f1_measure = f1_score( y_val,  model.predict(X_val), average="macro")
    print("F1_measure: ", f1_measure)

"""Si usano i dati divisi precedentemente, usando l'algoritmo Perceptron, per allineare un modello di classificazione. Il parametro *class_weight* permette di aumentare il peso degli errori nella classe con meno istanze (nel dataset1, la maggior parte delle istanze sono relative agli account reali). Come iperparametro *alpha* si è scelto quello che restituisce l'f1_measure maggiore."""

model_coeffs = []
i = 0

for alpha in np.logspace(-4, 0, 5):
    print("alpha {} :\n".format(alpha))
    model = Pipeline([("scaler", StandardScaler()), ("model",  Perceptron(random_state=42, penalty="l1", class_weight={1:5},alpha=alpha))
    ])
    fit(model, X_pre_train, y_pre_train, X_pre_val, y_pre_val)
    model_coeffs.append(get_coefficients(model, X_pre_train.columns))
    i += 1
    print("\n")

std_pen_model = Pipeline([("scaler", StandardScaler()), ("model",  Perceptron(random_state=42,penalty="l1", class_weight={1:5},alpha=0.0001))
    ])
fit(std_pen_model, X_pre_train, y_pre_train, X_pre_val, y_pre_val)

std_pen_model_coeff = get_coefficients(std_pen_model, X_pre_train.columns, 'model')
std_pen_model_coeff

"""L'analisi rivela che le feature *userBiographyLength* e *userIsPrivate* sono irrilevanti. Al contrario, il numero di follower, following e post influenzano molto nella classificazione.

Successivamente viene mostrata una heat map che, utilizzando la **Correlazione di Pearson**, visualizza tutte le correlazioni tra features.
"""

pearson_correlation = X_pre_train.corr(method='pearson')

fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)

sb.heatmap(pearson_correlation, annot=True)

plt.show()

"""Dal grafico non si evidenziano correlazioni.

Nel grafico a dispersione sottostante vengono rappresentate, nelle tre dimensione, le feature più rilevanti: *userFollowingCount*, *userHasProfilPic* e *userMediaCount*. Osserviamo come, effettivamente, gli account fake (in rosso) e reali (in blu) si distribuiscono in maniera ben diversa: gli account fake hanno un alto numero di following, un numero di post quasi nullo e pochi follower. Al contrario, i reali tendono ad avere un numero di seguiti basso ma un alto numero di follower e post.
"""

fig = plt.figure(figsize=(18, 18))
ax = fig.add_subplot(projection='3d')
x = "userFollowingCount"
z = "userFollowerCount"
y = "userMediaCount"
ax.set_xlabel(x)
ax.set_ylabel(y)
ax.set_zlabel(z)
tmp = dataset1
account_colors= tmp["isFake"].map(account_color_map)

ax.scatter(tmp[x], tmp[y], tmp[z], c=account_colors)

"""## Studio dataset2

Per lo studio delle feature più irrilevanti, vengono eseguiti gli stessi procedimenti già visti nel dataset1.
"""

y2 =dataset2["fake"]
x2 = dataset2.drop(["fake"], axis=1)

X_pre_train2, X_pre_val2, y_pre_train2, y_pre_val2 = train_test_split(
    x2,
    y2,
    test_size=1/3, random_state=42
)

model_coeffs = []
i = 0

for alpha in np.logspace(-4, 0, 5):
    print("alpha {} :\n".format(alpha))
    model = Pipeline([("scaler", StandardScaler()), ("model",  Perceptron(random_state=42,penalty="l1", alpha=alpha))
    ])
    fit(model, X_pre_train2, y_pre_train2, X_pre_val2, y_pre_val2)
    model_coeffs.append(get_coefficients(model, X_pre_train2.columns))
    i += 1
    print("\n")

std_pen_model2 = Pipeline([
    ("scaler", StandardScaler()),

    ("model",  Perceptron(random_state=42, penalty="l1", alpha=0.0001))
])
fit(std_pen_model2, X_pre_train2, y_pre_train2, X_pre_val2, y_pre_val2)

std_pen_coeff2 = get_coefficients(std_pen_model2, X_pre_train2.columns)
std_pen_coeff2

"""Le variabili più influenti (come ci si aspettava, avendole già studiate nella parte 1) sono *#followers*, *#posts* e *profile pic*."""

pearson_correlation = X_pre_train2.corr(method='pearson')

fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)

sb.heatmap(pearson_correlation, annot=True)

plt.show()

"""Dal grafico non si evidenziano rilevanti correlazioni.

Per effettuare il confronto con il dataset1, sono stati realizzati i seguenti grafici a dispersione, disponendo, negli assi, le stesse features. A destra c'è il grafico realtivo al dataset1 (già rappresentato in precedenza), a sinistra c'è quello relativo al secondo dataset. Osservandoli, la distribuzione degli account fake e reali al variare delle stesse feature è molto simile.
"""

fig = plt.figure(figsize=(25, 25))
ax = fig.add_subplot(2,2,1,projection='3d')
x = "#follows"
z = "#followers"
y = "#posts"
ax.set_xlabel(x)
ax.set_ylabel(y)
ax.set_zlabel(z)
tmp = dataset2[dataset2["#followers"] < 15000]
account_colors= tmp["fake"].map(account_color_map)
ax.scatter(tmp[x], tmp[y], tmp[z], c=account_colors)

ax = fig.add_subplot(2,2,2,projection='3d')
x = "userFollowingCount"
z = "userFollowerCount"
y = "userMediaCount"
ax.set_xlabel(x)
ax.set_ylabel(y)
ax.set_zlabel(z)
tmp = dataset1
account_colors= tmp["isFake"].map(account_color_map)

ax.scatter(tmp[x], tmp[y], tmp[z], c=account_colors)

"""# PARTE 3"""

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
import math
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix

"""La seguente funziona confronta il modello con i diversi iperparametri e ritorna il modello che ha restituito la f1_measure migliore. """

from sklearn.utils import class_weight
k_fold = KFold(n_splits=5, shuffle=True, random_state=42)
models = {}

def print_k_cross_validation_scores(model, X, y, kf):
    scores = cross_val_score(model, X, y, cv=kf)
    print("            Scores: {}\n              Mean: {}\nStandard deviation: {}".format(scores, scores.mean(), scores.std()))

def grid_search_with_cross_validation(model, grid, kf, dataset, class_attribute, scoring=None):
    grid_search = GridSearchCV(model, grid, scoring=scoring, cv=kf, n_jobs=-1)

    X_train, X_val, y_train, y_val = train_test_split(
        dataset.drop([class_attribute], axis=1),
        dataset[class_attribute],
        test_size=1/3, random_state=42
    )

    grid_search.fit(X_train, y_train)
    
    score = grid_search.score(X_val, y_val)

    print("Best cross validation score: {}\n".format(grid_search.best_score_))
    print("             Test set score: {}\n".format(score))
    print("                Best params: {}\n".format(grid_search.best_params_))
    print("             Best estimator: {}\n".format(grid_search.best_estimator_))
    return grid_search.best_estimator_, score
    #print(pd.DataFrame(grid_search.cv_results_))

"""Le seguenti funzioni verrranno usate per graficare i modelli, per ogni combinazione di features."""

def separator_2d(model, x1, nameModel, m, n):
    # ricaviamo w e b dal modello
    w = model.named_steps[nameModel].coef_[0]
    b = model.named_steps[nameModel].intercept_
    return -x1 * w[m] / w[n] - b / w[n]

def separator_3d(model, x1,y1, nameModel, i,j,t):
    # ricaviamo w e b dal modello
    w = model.named_steps[nameModel].coef_[0]
    b = model.named_steps[nameModel].intercept_
    return ((-x1 * w[i]) / w[t]) -((y1* w[j])/w[t]) - (b / w[t])

def draw_line(model, nameModel, i, j, color):
    xlim, ylim = plt.xlim(), plt.ylim()
    sep_x = np.linspace(*xlim, 2)
    sep_y = separator_2d(model, sep_x, nameModel, i , j)
    plt.plot(sep_x, sep_y, c=color, linewidth=2, label=nameModel)
    plt.xlim(xlim); plt.ylim(ylim)


def plot_separator_on_data(X, y, dataset,avoidList, model=None, nameModel=None,
                           repeatCombination = True, newFig = True,
                           lineColor = "green"):
    X = np.array(X)
    n, m = X.shape
    colors = pd.Series(y).map(account_color_map)
    if(newFig):
      plt.figure(figsize=(50, 50))
    k = 0
    for i in range(m): 
      g = 0 if repeatCombination else i+1
      for j in range(g,m):
        if(j!=i and i not in(avoidList) and j not in (avoidList)):
          k+=1
          plt.subplot(m, m, k)
          plt.scatter(X[:, i], X[:, j], c=colors)
          plt.xlabel(dataset.columns[i])
          plt.ylabel(dataset.columns[j])
          if model is not None:
            draw_line(model,nameModel, i, j, lineColor)

"""Di seguito viene definita una funzione che calcola tutti i dati utili per valutare un modello.
- Accuratezza: percentuale di classificazioni corrette (in caso di sbilanciamento tra classi, non molto utile)
- Precision: indica la percentuale di esempi classificati in una classe che sono realmente tali
- Recall: indica la percentuale di esempi realmente di una classe che sono stati classificati come tali
- F1-measure: media armonica tra precision e recall: misura unica della performance di un modello 
"""

def measureModel(x_val, y_val, y_pred, model):
  accuracy = model.score(x_val, y_val)
  precision_fake = precision_score(y_val, y_pred, pos_label=1)
  precision_real = precision_score(y_val, y_pred, pos_label=0)
  recall_fake = recall_score(y_val, y_pred, pos_label=1)
  recall_real = recall_score(y_val, y_pred, pos_label=0)
  f1_measure = f1_score(y_val, y_pred, average="macro")
  print("Accuracy: {}\n".format(accuracy))
  print("Precision fake: {}\n".format(precision_fake))
  print("Precision real: {}\n".format(precision_real))
  print("Recall fake: {}\n".format(recall_fake))
  print("Recall real: {}\n".format(recall_real))
  print("f1_measure: {}\n".format(f1_measure))

"""## Perceptron - Dataset1

Si utilizza il seguente metodo per ottenere la migliore combinazione di iperparametri sul metodo Perceptron, utilizzando il K-fold-cross-validation. Si usa il parametro *class_weight* per aumentare il peso degli errori nella classe con meno istanze.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# per_model = Pipeline([
#     ("scaler", StandardScaler()),
#     ("per", Perceptron(n_jobs=-1, random_state=42, class_weight={1:5}))
# ])
# 
# #print(per_model.get_params())
# 
# per_grid = {
#     "scaler": [None, StandardScaler()],
#     "per__penalty": ["l2", "l1", "elasticnet"],
#     "per__alpha": np.logspace(-3, 3, 7),
#     "per__fit_intercept": [False, True]
# }
# 
# per_model, score = grid_search_with_cross_validation(per_model, per_grid, k_fold, dataset1, "isFake", scoring = "f1_macro")
# models["Perceptron1"] = {"Model": per_model, "Score": score}
#

model = Pipeline(steps=[('scaler', None),
                ('per',
                 Perceptron(alpha=0.1, class_weight={1: 5}, fit_intercept=False,
                            n_jobs=-1, penalty='l1', random_state=42))])
model.fit(X_pre_train, y_pre_train)

f1_measure = f1_score( y_pre_val,  model.predict(X_pre_val), average="macro")
models["Perceptron1"] = {"Model": model, "Score": f1_measure}

y_pred = model.predict(X_pre_val)
measureModel(X_pre_val, y_pre_val, y_pred, model)

"""Il valore più basso tra quelli osservati è la precision degli account fake, ovvero la percentuale di account predetti come falsi che erano veramente falsi. Si osserva inoltre che la precision degli account reali è molto alta. La recall, ovvero la percentuale di account predetti correttamente sui totali, è alta in entrambe le classi.

Successivamente, si visualizzano i valori dei coefficienti a seguito dell'applicazione del modello. A causa della regolarizzazione L1, delle variabili hanno assunto valore nullo. Se andassimo a graficare, la retta non sarebbe visibile a causa della divisione per zero. Dunque, viene creato un array *not draw* che contiene l'indice dei coefficienti da non rappresentare. Questo procedimento verrà eseguito anche nei successivi modelli.
"""

print(model.named_steps["per"].coef_)
print(model.named_steps["per"].intercept_)

not_draw = [2,4, 5, 7]

plot_separator_on_data(X_pre_val, y_pre_val, dataset1,not_draw,model, "per")

"""## Perceptron - Dataset2

Si applica il Perceptron al dataset2 : in questo caso non si usa il parametro *class_weight* poichè le due classi sono perfettamente equilibrate.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# per_model = Pipeline([
#     ("scaler", StandardScaler()),
#     ("per", Perceptron(n_jobs=-1, random_state=42))
# ])
# 
# #print(per_model.get_params())
# 
# per_grid = {
#     "scaler": [None, StandardScaler()],
#     "per__penalty": ["l2", "l1", "elasticnet"],
#     "per__alpha": np.logspace(-3, 3, 7),
#     "per__fit_intercept": [False, True]
# }
# 
# per_model, score = grid_search_with_cross_validation(per_model, per_grid, k_fold, dataset2, "fake", scoring = "f1_macro")
# models["Perceptron2"] = {"Model": per_model, "Score": score}

model2 = Pipeline(steps=[('scaler', StandardScaler()),
                ('per',
                 Perceptron(alpha=0.001, n_jobs=-1, penalty='elasticnet',
                            random_state=42))])
model2.fit(X_pre_train2, y_pre_train2)
f1_measure = f1_score( y_pre_val2,  model2.predict(X_pre_val2), average="macro")

models["Perceptron2"] = {"Model": model2, "Score": f1_measure}

y_pred2 = model2.predict(X_pre_val2)
measureModel(X_pre_val2, y_pre_val2, y_pred2, model2)

"""Si nota che la precision degli account fake è molto alta, quello dei reali è più bassa (al contrario di come accadeva nel dataset1). Viceversa, la recall dei reali è più alta di quelli fake."""

model2.named_steps["per"].coef_

"""Le feature che non verranno visualizzate nel grafico (perchè con la regolarizzazione sono azzerate) sono: *nums/length fullname* e *private*.
Inoltre anche la feature **#followers** non verrá visualizzata a causa della sua notevole differenza di grandezza rispetto al resto delle feature.
"""

not_draw = [3,7,9]

plot_separator_on_data(X_pre_val2, y_pre_val2, dataset2 ,not_draw, model2, "per")

"""## Regressione Logistica - Dataset1

Si passa ora a studiare la regressione logistica. I procedimenti sono gli stessi visti per il Perceptron.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# log_model = Pipeline([
#     ("scaler", StandardScaler()),
#     ("lr", LogisticRegression(solver='liblinear', class_weight={1:5}, random_state=42))
# ])
# 
# #print(log_model.get_params())
# 
# log_grid = {
#     "scaler": [None, StandardScaler()],
#     "lr__penalty": ["l2", "l1"],
#     "lr__C": np.logspace(-4, 2, 7),
#     "lr__fit_intercept": [False, True]
# }
# 
# log_model, score = grid_search_with_cross_validation(log_model, log_grid, k_fold, dataset1,"isFake", scoring = "f1_macro")
# models["Logistic Regression1"] = {"Model" : log_model, "Score": score}

log_model = Pipeline(steps=[('scaler', None),
                ('lr',
                 LogisticRegression(C=0.001, class_weight={1: 5},penalty="l1",
                                    fit_intercept=False,
                                    random_state=42, solver='liblinear'))])
log_model.fit(X_pre_train, y_pre_train)
f1_measure = f1_score( y_pre_val,  log_model.predict(X_pre_val), average="macro")
models["Logistic Regression1"] = {"Model" : log_model, "Score": f1_measure}

measureModel(X_pre_val, y_pre_val, log_model.predict(X_pre_val), log_model)

"""Si nota come la precision fake sia molto bassa, mentre quella dei reali sia molto alta (come nel Perceptron). Le recall sono alquanto simili tra loro mentre la f1_measure risulta migliore del Perceptron."""

log_model.named_steps["lr"].coef_

not_draw = [2,4,5,6,7]

plot_separator_on_data(X_pre_val, y_pre_val,dataset1,not_draw,log_model, "lr")

"""## Regressione logistica - Dataset 2"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# log_model = Pipeline([
#     ("scaler", StandardScaler()),
#     ("lr", LogisticRegression(solver='liblinear', random_state=42))
# ])
# 
# #print(log_model.get_params())
# 
# log_grid = {
#     "scaler": [None, StandardScaler()],
#     "lr__penalty": ["l2", "l1"],
#     "lr__C": np.logspace(-4, 2, 7),
#     "lr__fit_intercept": [False, True]
# }
# 
# log_model, score = grid_search_with_cross_validation(log_model, log_grid, k_fold, dataset2,"fake", scoring = "f1_macro")
# models["Logistic Regression2"] = {"Model" : log_model, "Score": score}

log_model2 =Pipeline(steps=[('scaler', None),
                ('lr',
                 LogisticRegression(penalty='l1', random_state=42,
                                    solver='liblinear'))])

log_model2.fit(X_pre_train2, y_pre_train2)
f1_measure = f1_score( y_pre_val2,  log_model2.predict(X_pre_val2), average="macro")

models["Logistic Regression2"] = {"Model" : log_model2, "Score": f1_measure}

measureModel(X_pre_val2, y_pre_val2, log_model2.predict(X_pre_val2), log_model2)

"""Si nota come, equivalentemente al Perceptron, la precision dei fake risulta più alta dei reali mentre la recall fake è più bassa della recall real. La f1_measure rivela un netto miglioramento della Logistic Regression rispetto al Perceptron."""

log_model2.named_steps["lr"].coef_

not_draw = [3]

"""Per visualizzare meglio i grafici, vengono filtrate e rappresentate solo le istanze con un numero di followers minore di 10000."""

y2f =dataset2[dataset2["#followers"] < 10000]["fake"]
x2f = dataset2[dataset2["#followers"] < 10000].drop(["fake"], axis=1)
i1, i2, i3, i4 = train_test_split(
    x2f,y2f,test_size=1/3, random_state=42)
plot_separator_on_data(i2, i4, dataset2 ,not_draw, log_model2, "lr")

"""## Support Vector Machine - Dataset1 """

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# svm_model = Pipeline([
#     ("scaler", StandardScaler()),
#     ("svc", SVC(random_state=42, class_weight={1:5}))
# ])
# 
# #print(svm_model.get_params())
# 
# svm_grid = [
#   {'svc__C': np.logspace(3, 5, 3), 'svc__kernel': ['linear']},
#   {'svc__C': np.logspace(3, 5, 3), 'svc__gamma': ['scale'], 'svc__kernel': ['rbf']},
# ]
# 
# svm_model, score= grid_search_with_cross_validation(svm_model, svm_grid, k_fold,  dataset1, "isFake",  scoring = "f1_macro")
# models["Support Vector Machine1"] = {"Model": svm_model, "Score": score}

svm_model = Pipeline(steps=[('scaler', StandardScaler()),
                ('svc',
                 SVC(C=10000.0, class_weight={1: 5}, kernel='linear',
                     random_state=42))])
svm_model.fit(X_pre_train, y_pre_train)
f1_measure = f1_score( y_pre_val,  svm_model.predict(X_pre_val), average="macro")
models["Support Vector Machine1"] = {"Model": svm_model, "Score": f1_measure}

measureModel(X_pre_val, y_pre_val, svm_model.predict(X_pre_val), svm_model)

"""Questi dati rivelano un peggioramento rispetto ai modelli visualizzati in precedenza: la precision fake in particolare ha infatti un valore molto più basso di quelli visti negli altri modelli. La f1_measure risulta essere la peggiore misurata fino ad ora."""

svm_model.named_steps["svc"].coef_

plot_separator_on_data(X_pre_val, y_pre_val, dataset1, [], svm_model, "svc")

"""Dal grafico si nota che non in tutti i quadranti è presente la retta: questo perchè i coefficienti di entrambe le features sono negativi e la retta non passa per il primo quadrante.

## Support Vector Machine - Dataset2
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# svm_model = Pipeline([
#     ("scaler", StandardScaler()),
#     ("svc", SVC(random_state=42))
# ])
# 
# #print(svm_model.get_params())
# 
# svm_grid = [
#   {'svc__C': np.logspace(3, 5, 3), 'svc__kernel': ['linear']},
#   {'svc__C': np.logspace(3, 5, 3), 'svc__gamma': ['scale'], 'svc__kernel': ['rbf']},
# ]
# 
# svm_model, score= grid_search_with_cross_validation(svm_model, svm_grid, k_fold,  dataset2, "fake",  scoring = "f1_macro")
# models["Support Vector Machine2"] = {"Model": svm_model, "Score": score}

svm_model2 = Pipeline(steps=[('scaler', StandardScaler()),
                ('svc', SVC(C=100000.0, kernel='linear', random_state=42))])
svm_model2.fit(X_pre_train2, y_pre_train2)
f1_measure = f1_score( y_pre_val2,  svm_model2.predict(X_pre_val2), average="macro")
models["Support Vector Machine2"] = {"Model": svm_model2, "Score": f1_measure}

measureModel(X_pre_val2, y_pre_val2, svm_model2.predict(X_pre_val2), svm_model2)

"""Al contrario di quanto visto per il dataset1, il SVM ha restituito un buon valore di f1_measure, leggermente inferiore a quello ottimo rilevato della regressione logistica. Gli altri valori, ad eccezione della recall fake che è leggermente più bassa, sono tutti alti."""

svm_model2.named_steps["svc"].intercept_

svm_model2.named_steps["svc"].coef_

y2f =dataset2[dataset2["#followers"] < 5000]["fake"]
x2f = dataset2[dataset2["#followers"] < 5000].drop(["fake"], axis=1)
i1, i2, i3, i4 = train_test_split(
    x2f,y2f,test_size=1/3, random_state=42)
plot_separator_on_data(i2, i4, dataset2 ,[], svm_model2, "svc")

"""Non in tutti i grafici è visualizzabile la retta: ciò è dovuto a un valore dell'intercetta molto basso e al fatto che alcuni coefficienti hanno valore negativo.

# PARTE 4
"""

from scipy import stats

"""## Dataset1

Il seguente metodo permette di confrontare i diversi modelli con intervallo di confidenza al 95%.
"""

def difference_between_two_models(error1, error2, confidence, dataset, y_val):
    z_half_alfa = stats.norm.ppf(confidence)
    variance = (((1 - error1) * error1) / len(y_val)) + (((1 - error2) * error2) / len(y_val))
    d_minus = abs(error1 - error2) - z_half_alfa * (pow(variance, 0.5))
    d_plus = abs(error1 - error2) + z_half_alfa * (pow(variance, 0.5))
    print("Valore minimo: {}\nValore massimo: {}\n".format(d_minus, d_plus))

per_error = 1 - models["Perceptron1"]["Score"]
lre_error = 1 - models["Logistic Regression1"]["Score"]
svm_error = 1 - models["Support Vector Machine1"]["Score"]

print("Support Vector Machine vs Logistic Regression, intervallo di confidenza:")
difference_between_two_models(svm_error, lre_error, 0.95, dataset1, y_pre_val)

print("Support Vector Machine vs Perceptron, intervallo di confidenza:")
difference_between_two_models(svm_error, per_error, 0.95, dataset1, y_pre_val)

print("Logistic Regression vs Perceptron, intervallo di confidenza:")
difference_between_two_models( lre_error, per_error, 0.95, dataset1, y_pre_val)

"""Si nota una somiglianza tra Logistic Regression e Perceptron, come si evidenzia anche dall'f1_measure molto simile."""

print("F1_measure Perceptron:", models["Perceptron1"]["Score"])
print("F1_measure Logistic Regression: ", models["Logistic Regression1"]["Score"])
print("F1_measure Support Vector Machine: ", models["Support Vector Machine1"]["Score"])

"""Successivamente vengono generate le matrice di confusione dei diversi modelli."""

def confusion_matrix_calculation(model):
    return confusion_matrix(y_pre_val, model.predict(X_pre_val))

"""Matrice di confusione del Perceptron."""

model = Pipeline(steps=[('scaler', None),
                ('per',
                 Perceptron(alpha=0.1, class_weight={1: 5}, fit_intercept=False,
                            n_jobs=-1, penalty='l1', random_state=42))])

model.fit(X_pre_train, y_pre_train)
conf_perc = pd.DataFrame(confusion_matrix_calculation(model))
conf_perc

measureModel(X_pre_val, y_pre_val, model.predict(X_pre_val), model)

"""Matrice di confusione del Logistic Regression."""

conf_log_reg = pd.DataFrame(confusion_matrix_calculation(log_model))
conf_log_reg

measureModel(X_pre_val, y_pre_val, model.predict(X_pre_val), log_model)

"""Matrice di confidenza Support Vector Machine."""

conf_SVM = pd.DataFrame(confusion_matrix_calculation(svm_model))
conf_SVM

measureModel(X_pre_val, y_pre_val, model.predict(X_pre_val), svm_model)

"""Adesso vengono calcolati gli intervalli di confidenza al 95% per ogni modello."""

def confidence(acc, N, Z):
    den = (2*(N+Z**2))
    var = (Z*np.sqrt(Z**2+4*N*acc-4*N*acc**2)) / den
    a = (2*N*acc+Z**2) / den
    inf = a - var
    sup = a + var
    return (inf, sup)

def calculate_accuracy(conf_matrix):
    return np.diag(conf_matrix).sum() / conf_matrix.sum().sum()

#con confidenza del 0.95 si ha Z=1.96
pd.DataFrame([confidence(calculate_accuracy(conf_perc), len(X_pre_val), 1.96),
              confidence(calculate_accuracy(conf_log_reg), len(X_pre_val), 1.96),
              confidence(calculate_accuracy(conf_SVM), len(X_pre_val), 1.96)],
                 index=["perceptron", "logreg", "SVM"], columns=["inf", "sup"])

"""Dagli intervalli di confidenza al 95% abbiamo la conferma che la Logistic Regression è il modello più accurato.

Di seguito possiamo confrontare le rette create utilizzando i diversi modelli, sovrapposte al grafico considerando le feature: numero di followers, numero di follows. 
* linea verde: Perceptron
* linea viola: Regressione logistica
* linea nera: Support vector machine
"""

hidden = [2,3,4,5,6,7]

plt.figure(figsize=(70, 70))
plot_separator_on_data(X_pre_val, y_pre_val, dataset1 ,hidden, models["Perceptron1"]["Model"], "per", repeatCombination=False, newFig = False, lineColor="green")
plot_separator_on_data(X_pre_val, y_pre_val, dataset1 ,hidden, models["Logistic Regression1"]["Model"], "lr", repeatCombination=False, newFig = False, lineColor="violet")
plot_separator_on_data(X_pre_val, y_pre_val, dataset1 ,hidden, models["Support Vector Machine1"]["Model"], "svc", repeatCombination=False, newFig = False, lineColor="black")

"""Ricordiamo che, dalla misura dell'f1_measure, il modello migliore è Logistic Regression, seguito da Perceptron e da Support Vector Machine.
Di seguito un grafico tridimensionale che rappresenta il piano ottenuto con logistic regression.

"""

fig = plt.figure(figsize=(20, 20))
ax = fig.gca(projection='3d')

tmp = dataset1
xi = 0
yi = 1
zi = 3
x = tmp.columns[xi]
y = tmp.columns[yi]
z = tmp.columns[zi]

fake =  "isFake"
ax.set_xlabel(x)
ax.set_ylabel(y)
ax.set_zlabel(z)

account_colors= tmp[fake].map(account_color_map)
# Plot Curve Fit
# Create dataframe with of x_fit and y_fit

#print(tmp)

# Pass to the model's predict() method to return z-fit


#print(zz)

#ax.set_zlim(0,10000)

ax.scatter(tmp[x], tmp[y], tmp[z], c=account_colors)
xlim, ylim = plt.xlim(), plt.ylim()

X = np.arange(0, tmp[x].max(), tmp[x].max()/2)
Y = np.arange(0,tmp[y].max(), tmp[y].max()/2)
X, Y = np.meshgrid(X, Y)
Z = separator_3d(log_model, X, Y,"lr",xi,yi,zi)

Z = np.asarray(Z).reshape(-1, 2)

surf = ax.plot_surface(X,Y,Z, linewidth=0, antialiased=False, alpha = 0.2)
plt.show()

"""## Dataset2

Si effettuano gli stessi passaggi visti per il dataset1, per studiare il dataset2.
"""

def difference_between_two_models(error1, error2, confidence, y_val):
    z_half_alfa = stats.norm.ppf(confidence)
    variance = (((1 - error1) * error1) / len(y_val)) + (((1 - error2) * error2) / len(y_val))
    d_minus = abs(error1 - error2) - z_half_alfa * (pow(variance, 0.5))
    d_plus = abs(error1 - error2) + z_half_alfa * (pow(variance, 0.5))
    print("Valore minimo: {}\nValore massimo: {}\n".format(d_minus, d_plus))

per_error = 1 - models["Perceptron2"]["Score"]
lre_error = 1 - models["Logistic Regression2"]["Score"]
svm_error = 1 - models["Support Vector Machine2"]["Score"]

print("Support Vector Machine vs Logistic Regression, intervallo di confidenza:")
difference_between_two_models(svm_error, lre_error, 0.95,  y_pre_val2)

print("Support Vector Machine vs Perceptron, intervallo di confidenza:")
difference_between_two_models(svm_error, per_error, 0.95, y_pre_val2)

print("Logistic Regression vs Perceptron, intervallo di confidenza:")
difference_between_two_models( lre_error, per_error, 0.95, y_pre_val2)

"""Dallo studio degli intervalli di confidenza, i modelli che sono più simili sono SVM (f1_score=0.930) e Logistic Regression(f1_score=0.935).



"""

print("F1_measure Perceptron: ", models["Perceptron2"]["Score"])
print("F1_measure Logistic Regression: ", models["Logistic Regression2"]["Score"])
print("F1_measure Support Vector Machine: ", models["Support Vector Machine2"]["Score"])

"""Il modello migliore del dataset2 è Logistic Regression. Di seguito vengono rappresentate le matrici di confidenza."""

def confusion_matrix_calculation(model):
    return confusion_matrix(y_pre_val2, model.predict(X_pre_val2))

"""Matrice di confidenza del perceptron."""

conf_perc2 = pd.DataFrame(confusion_matrix_calculation(model2))
conf_perc2

measureModel(X_pre_val2, y_pre_val2, model2.predict(X_pre_val2), model2)

"""Matrice di confidenza di Logistic Regression."""

conf_log_reg2 = pd.DataFrame(confusion_matrix_calculation(log_model2))
conf_log_reg2

measureModel(X_pre_val2, y_pre_val2, log_model2.predict(X_pre_val2), model2)

"""Matrice di confidenza di Support Vector Machine."""

conf_SVM2 = pd.DataFrame(confusion_matrix_calculation(svm_model2))
conf_SVM2

measureModel(X_pre_val2, y_pre_val2, svm_model2.predict(X_pre_val2), model2)

#con confidenza del 0.95 si ha Z=1.96
pd.DataFrame([confidence(calculate_accuracy(conf_perc2), len(X_pre_val2), 1.96),
              confidence(calculate_accuracy(conf_log_reg2), len(X_pre_val2), 1.96),
              confidence(calculate_accuracy(conf_SVM2), len(X_pre_val2), 1.96)],
                 index=["perceptron", "logreg", "SVM"], columns=["inf", "sup"])

"""Dagli intervalli di confidenza al 95% abbiamo la conferma che Logistic Regression è il modello più accurato.

E' possible confrontare le rette realtive ai 3 modelli, sovrapponendole al grafico generato utilizzando alcune feature del dataset.

* linea verde: Perceptron
* linea viola: Regressione logistica
* linea nera: Support vector machines
"""

y2f =dataset2[dataset2["#followers"] < 5000]["fake"]
x2f = dataset2[dataset2["#followers"] < 5000].drop(["fake"], axis=1)
i1, i2, i3, i4 = train_test_split(
    x2f,y2f,test_size=1/3, random_state=42)
hidden = [0,1,2,3,4,5,6,7,10]
plt.figure(figsize=(150, 150))
plot_separator_on_data(i2, i4, dataset2 ,hidden, models["Perceptron2"]["Model"], "per", repeatCombination=False, newFig = False, lineColor="green")
plot_separator_on_data(i2, i4, dataset2 ,hidden, models["Logistic Regression2"]["Model"], "lr", repeatCombination=False, newFig = False, lineColor="violet")
plot_separator_on_data(i2, i4, dataset2 ,hidden, models["Support Vector Machine2"]["Model"], "svc", repeatCombination=False, newFig = False, lineColor="black")
plt.figure(figsize=(150, 150))
hidden = [0,1,2,3,4,5,6,7,9]
plt.figure(figsize=(150, 150))
plot_separator_on_data(i2, i4, dataset2 ,hidden, models["Perceptron2"]["Model"], "per", repeatCombination=False, newFig = False, lineColor="green")
plot_separator_on_data(i2, i4, dataset2 ,hidden, models["Logistic Regression2"]["Model"], "lr", repeatCombination=False, newFig = False, lineColor="violet")
plot_separator_on_data(i2, i4, dataset2 ,hidden, models["Support Vector Machine2"]["Model"], "svc", repeatCombination=False, newFig = False, lineColor="black")
plt.figure(figsize=(150, 150))

"""Ricordiamo che, dalla misura dell'f1_measure, il modello migliore è Logistic Regression, seguito da Support Vector Machine e da Perceptron. Di seguito il grafico tridimensionale che rappresenta il piano ottenuto con il modello migliore.


"""

fig = plt.figure(figsize=(20, 20))
ax = fig.gca(projection='3d')

tmp = dataset2
xi = 1
yi = 8
zi = 10
x = tmp.columns[xi]
y = tmp.columns[yi]
z = tmp.columns[zi]

fake =  "fake"
ax.set_xlabel(x)
ax.set_ylabel(y)
ax.set_zlabel(z)

account_colors= tmp[fake].map(account_color_map)
# Plot Curve Fit
# Create dataframe with of x_fit and y_fit

#print(tmp)

# Pass to the model's predict() method to return z-fit


#print(zz)

#ax.set_zlim(0,10000)

ax.scatter(tmp[x], tmp[y], tmp[z], c=account_colors)
xlim, ylim = plt.xlim(), plt.ylim()

X = np.arange(0, tmp[x].max(), tmp[x].max()/2)
Y = np.arange(0,tmp[y].max(), tmp[y].max()/2)
X, Y = np.meshgrid(X, Y)
Z = separator_3d(log_model2, X, Y,"lr",xi,yi,zi)

Z = np.asarray(Z).reshape(-1, 2)

surf = ax.plot_surface(X,Y,Z, linewidth=0, antialiased=False, alpha = 0.2)
plt.show()

"""# PARTE 5

## Dataset1

Si effettua uno studio sui coefficienti del modello che ha restituito il miglior risultato: logistic regression.
"""

log_model = Pipeline(steps=[('scaler', None),
                ('lr',
                 LogisticRegression(C=0.001, class_weight={1: 5},penalty="l1",
                                    fit_intercept=False,
                                    random_state=42, solver='liblinear'))])
log_model.fit(X_pre_train, y_pre_train)
log_model_coeff = get_coefficients(log_model, X_pre_train.columns, "lr")
log_model_coeff

min_values = dataset1.min()
max_values = dataset1.max()
denormalized_df= log_model_coeff * (max_values - min_values) + min_values
denormalized_df

"""A seguito della denormalizzazione, si nota, dallo studio dei coefficienti, che molte variabili vengono azzerate. Quella più rilevante è il numero di media, seguita dal numero di follower e di following. Ci si aspetta che un account con molti post e molti follower è, con buona probabilità, un account reale. Al contrario, se un account ha molti seguiti, un username lungo, pochi follower e pochi post, molto probabilmente sarà un account fake.

## Dataset2

Si effettua lo studio dei coefficienti anche sul dataset2. Il miglior risultato dell'f1_score è stato ottenuto, anche in questo caso, applicando la regressione logistica.
"""

log_model2 =Pipeline(steps=[('scaler', None),
                ('lr',
                 LogisticRegression(penalty='l1', random_state=42,
                                    solver='liblinear'))])

log_model2.fit(X_pre_train2, y_pre_train2)

log_model_coeff2 = get_coefficients(log_model2, X_pre_train2.columns, "lr")
log_model_coeff2

min_values = dataset2.min()
max_values = dataset2.max()
denormalized_df= log_model_coeff2 * (max_values - min_values) + min_values
denormalized_df

"""Dopo aver denormalizzato, come già osservato precedentemente, risulta che la variabile più influente è il numero di follower, seguita dal numero di post e dal numero di follows. Ci si aspetta che un account con molti follower è quasi sicuramente reale. Al contrario, se ha pochi follower, pochi post e molti seguiti, allora l'account sarà falso.

## Nuovo dataset2

Studiando il dataset2, si nota che ci sono valori molto alti per il numero di followers: questo potrebbe peggiorare il modello. Si vuole provare, dunque, a eliminare dal dataset queste variabili e riaddestrare i modelli.
"""

new_dataset2 = dataset2[dataset2["#followers"]< 10000]

new_y2 =new_dataset2["fake"]
new_x2 = new_dataset2.drop(["fake"], axis=1)

new_X_pre_train2, new_X_pre_val2, new_y_pre_train2, new_y_pre_val2 = train_test_split(
    new_x2,
    new_y2,
    test_size=1/3, random_state=42
)

"""### Perceptron"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# per_model = Pipeline([
#     ("scaler", StandardScaler()),
#     ("per", Perceptron(n_jobs=-1, random_state=42))
# ])
# 
# #print(per_model.get_params())
# 
# per_grid = {
#     "scaler": [None, StandardScaler()],
#     "per__penalty": ["l2", "l1", "elasticnet"],
#     "per__alpha": np.logspace(-3, 3, 7),
#     "per__fit_intercept": [False, True]
# }
# 
# per_model, score = grid_search_with_cross_validation(per_model, per_grid, k_fold, new_dataset2, "fake", scoring = "f1_macro")
# models["Perceptron2_new"] = {"Model": per_model, "Score": score}

new_per_model2 =Pipeline(steps=[('scaler', StandardScaler()),
                ('per',
                 Perceptron(alpha=0.001, fit_intercept=False, n_jobs=-1,
                            penalty='l2', random_state=42))])
new_per_model2.fit(new_X_pre_train2, new_y_pre_train2)
f1Measure = f1_score(new_y_pre_val2, new_per_model2.predict(new_X_pre_val2),average="macro")
models["Perceptron2_new"] = {"Model": new_per_model2, "Score": f1Measure}

new_std_pen_coeff2 = get_coefficients(new_per_model2, new_X_pre_train2.columns, "per")
new_std_pen_coeff2

measureModel(new_X_pre_val2, new_y_pre_val2, new_per_model2.predict(new_X_pre_val2), new_per_model2)

"""Si nota già che la f1_measure è migliorata: nel dataset2 originale valeva 0.900, ora 0.912.

### Logistic Regression
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# log_model = Pipeline([
#     ("scaler", StandardScaler()),
#     ("lr", LogisticRegression(solver='liblinear', random_state=42))
# ])
# 
# #print(log_model.get_params())
# 
# log_grid = {
#     "scaler": [None, StandardScaler()],
#     "lr__penalty": ["l2", "l1"],
#     "lr__C": np.logspace(-4, 2, 7),
#     "lr__fit_intercept": [False, True]
# }
# 
# log_model, score = grid_search_with_cross_validation(log_model, log_grid, k_fold, new_dataset2,"fake", scoring = "f1_macro")
# models["Logistic Regression2_new"] = {"Model" : log_model, "Score": score}

new_log_model2 = Pipeline(steps=[('scaler', StandardScaler()),
                ('lr',
                 LogisticRegression(C=0.0001, random_state=42,
                                    solver='liblinear'))])
new_log_model2.fit(new_X_pre_train2, new_y_pre_train2)
f1Measure = f1_score(new_y_pre_val2, new_log_model2.predict(new_X_pre_val2),average="macro")
models["Logistic regression2_new"] = {"Model": new_log_model2, "Score": f1Measure}

new_std_pen_coeff2 = get_coefficients(new_log_model2, new_X_pre_train2.columns, "lr")
new_std_pen_coeff2

measureModel(new_X_pre_val2, new_y_pre_val2, new_log_model2.predict(new_X_pre_val2), new_log_model2)

"""Con la logistic regressione, la f1_measure è peggiorata di pochissimo.

### Support Vector Machine
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# svm_model = Pipeline([
#     ("scaler", StandardScaler()),
#     ("svc", SVC(random_state=42))
# ])
# 
# #print(svm_model.get_params())
# 
# svm_grid = [
#   {'svc__C': np.logspace(3, 5, 3), 'svc__kernel': ['linear']},
#   {'svc__C': np.logspace(3, 5, 3), 'svc__gamma': ['scale'], 'svc__kernel': ['rbf']},
# ]
# 
# svm_model, score= grid_search_with_cross_validation(svm_model, svm_grid, k_fold,  new_dataset2, "fake",  scoring = "f1_macro")
# models["Support Vector Machine2_new"] = {"Model": svm_model, "Score": score}

new_svm_model2 = Pipeline(steps=[('scaler', StandardScaler()),
                ('svc', SVC(C=1000.0, kernel='linear', random_state=42))])
new_svm_model2.fit(new_X_pre_train2, new_y_pre_train2)
f1Measure = f1_score(new_y_pre_val2, new_svm_model2.predict(new_X_pre_val2),average="macro")
models["Support Vector Machine2_new"] = {"Model": new_svm_model2, "Score": f1Measure}
fit(new_svm_model2, new_X_pre_train2, new_y_pre_train2, new_X_pre_val2, new_y_pre_val2)

"""La support vectore machine è migliorata notevolmente: col nuovo dataset l'f1_measure vale 0.945, prima era 0.9309."""

measureModel(new_X_pre_val2, new_y_pre_val2, new_svm_model2.predict(new_X_pre_val2), new_svm_model2)

new_std_pen_coeff2 = get_coefficients(new_svm_model2, new_X_pre_train2.columns, "svc")
new_std_pen_coeff2

"""Confrontiamo le rette dei 3 modelli sovrapposte al grafico considerando le feature #followers, #follows"""

hidden =[0,1,2,3,4,5,6,7,8]

plt.figure(figsize=(150, 150))
plot_separator_on_data(new_X_pre_val2, new_y_pre_val2, new_dataset2, hidden, models["Perceptron2_new"]["Model"], 
                       "per", lineColor="green", newFig= False, repeatCombination=False)
plot_separator_on_data(new_X_pre_val2, new_y_pre_val2, new_dataset2, hidden, models["Logistic regression2_new"]["Model"],
                       "lr", lineColor="violet", newFig= False, repeatCombination=False)
plot_separator_on_data(new_X_pre_val2, new_y_pre_val2, new_dataset2, hidden, models["Support Vector Machine2_new"]["Model"],
                       "svc", lineColor="black", newFig= False, repeatCombination=False)

"""# Dataset3

Il dataset3 viene creato combinando i due dataset precedentemente utilizzati. Per farlo, vengono rinominate alcune features e fatto un merge sulle colonne in comune.
"""

dataset3 = dataset1.copy()

dataset3["nums/length username"] = round(dataset3["usernameDigitCount"] / dataset3["usernameLength"] ,2)
dataset3.rename(columns = {'userFollowerCount':'#followers', 'userFollowingCount':'#follows', 'userBiographyLength': 'description length',
                           'userMediaCount': '#posts','userHasProfilPic': 'profile pic', 'userIsPrivate':'private', 'isFake':'fake' }, inplace = True)
dataset3 = dataset3.drop(columns = ["usernameDigitCount", "usernameLength"])
dataset3 = pd.concat([new_dataset2, dataset3], ignore_index=True, join='inner')

dataset3

dataset3[dataset3["fake"]==0].value_counts()

dataset3[dataset3["fake"]==1].value_counts()

"""Il nuovo dataset ha in totale 1890 istanze, di queste 1312 sono account reali. Nel seguente grafico si visualizza la distribuzione delle due classi (la parte blu rappresenta gli account reali)."""

dataset3['fake'].value_counts().plot.pie(autopct="%1.1f%%")

"""## PARTE 1

## Significato delle feature
"""

plt.figure(figsize=(30,30))

plt.subplot(4,2,1)
plt.title("numbers/ (length username)")
plt.hist(dataset3["nums/length username"], color="yellow")
plt.ylabel("count")
plt.xlabel("ratio")

plt.subplot(4,2,2)
plt.title("Description length")
plt.hist(dataset3["description length"], color="purple")
plt.ylabel("count")
plt.xlabel("Description length")

plt.subplot(4,2,3)
plt.title("Number of posts")
plt.hist(dataset3["#posts"], color="green")
plt.ylabel("count")
plt.xlabel("Number of posts")

plt.subplot(4,2,4)
plt.title("Number of followers")
plt.hist(dataset3["#followers"], color="blue")
plt.ylabel("count")
plt.xlabel("Number of followers")

plt.subplot(4,2,5)
plt.title("Number of follows")
plt.hist(dataset3["#follows"], color="grey")
plt.ylabel("count")
plt.xlabel("Number of follows")

plt.figure(figsize=(20,20))

plt.subplot(4,2,1)
plt.title("numbers/ (length username)")
plt.boxplot(dataset2["nums/length username"])
plt.ylabel("ratio")

plt.subplot(4,2,2)
plt.title("Description length")
plt.boxplot(dataset2["description length"])
plt.ylabel("Description length")

plt.subplot(4,2,3)
plt.title("Number of posts")
plt.boxplot(dataset2["#posts"])
plt.ylabel("Number of posts")

plt.subplot(4,2,4)
plt.title("Number of followers")
plt.boxplot(dataset2["#followers"])
plt.ylabel("Number of followers")

plt.subplot(4,2,5)
plt.title("Number of follows")
plt.boxplot(dataset2["#follows"])
plt.ylabel("Number of follows")

"""Analizziamo alcune variabili e le loro correlazioni con la variabile **fake**

**Profile pic**: osserviamo se la presenza di una foto profilo è più riccorrente negli account veri rispetto a quelli falsi
"""

fakeWithProfile = dataset3[(dataset3['fake'] == 1) & (dataset3['profile pic'] == 1)].shape[0]
fakeNoProfile = dataset3[(dataset3['fake'] == 1) & (dataset3['profile pic'] == 0)].shape[0]
realWithProfile = dataset3[(dataset3['fake'] == 0) & (dataset3['profile pic'] == 1)].shape[0]
realNoProfile = dataset3[(dataset3['fake'] == 0) & (dataset3['profile pic'] == 0)].shape[0]

df = pd.DataFrame({'with profile pic':[fakeWithProfile, realWithProfile],
                   'without profile pic':[fakeNoProfile, realNoProfile]}, 
                  index=['Fake accounts','Real accounts'])
df.T.plot.pie(subplots=True, figsize=(10, 30))

"""Possiamo osservare che quasi tutti gli account veri hanno una foto profilo.
Negli account falsi circa la meta'. Si può dedurre che un account senza foto profilo anche all'interno di questo dataset quasi sicuramente sarà un account falso.
"""

print("Fake with no profile:" , fakeNoProfile)
print("Real with no profile:" , realNoProfile)
print("Fake probability:" , (fakeNoProfile / (fakeNoProfile + realNoProfile)) * 100, "%")

"""Possiamo notare che un account senza foto profilo all'interno del dataset ha una probabilità dell 95% di essere fake.

**Follows and followers number**: Confrontiamo la relazione tra numero di follower e di follows tra account reali e account falsi. Ci aspettiamo che quelli falsi abbiano un numero di seguiti maggiore rispetto ai seguaci.
"""

account_color_map ={0:"blue", 1:"red"}
account_colors= dataset3["fake"].map(account_color_map)
tmp = dataset3

tmp.plot.scatter("#followers", "#follows", c=tmp["fake"].map(account_color_map), figsize=(10,10))

"""Come previsto gli account falsi hanno un un numero maggiore di follows e pochi followers. Negli account veri si nota il contrario, tranne per alcune eccezioni dove il numero di followers é molto elevato (probabilmente si tratta di account molto famosi). Queste eccezioni sono state escluse dal grafico per facilitarne la visualizzazione.

## PARTE 2

Si effettua lo studio sui coefficienti più rilevanti del dataset3, procedendo come per gli altri due dataset.
"""

y3 = dataset3["fake"]
x3 = dataset3.drop(["fake"], axis=1)

X_pre_train3, X_pre_val3, y_pre_train3, y_pre_val3 = train_test_split(
    x3,
    y3,
    test_size=1/3, random_state=42
)

def get_coefficients(model, index, model_name="model"):
    return pd.Series(model.named_steps[model_name].coef_[0], index=index)
    
def fit(model, X_train, y_train, X_val, y_val):
    model.fit(X_train, y_train)
    f1_measure = f1_score( y_val,  model.predict(X_val), average="macro")
    print("F1_measure: ", f1_measure)

model_coeffs = []
i = 0

for alpha in np.logspace(-4, 0, 5):
    print("alpha {} :\n".format(alpha))
    model = Pipeline([("scaler", StandardScaler()), ("model",  Perceptron(random_state=42, penalty="l1", class_weight={1:3},alpha=alpha))
    ])
    fit(model, X_pre_train3, y_pre_train3, X_pre_val3, y_pre_val3)
    model_coeffs.append(get_coefficients(model, X_pre_train3.columns))
    i += 1
    print("\n")

std_pen_model3 = Pipeline([
    ("scaler", StandardScaler()),
    ("model",  Perceptron( random_state=42, class_weight={1:3}, penalty="l1", alpha=0.001))
])

fit(std_pen_model3, X_pre_train3, y_pre_train3, X_pre_val3, y_pre_val3)

std_pen_model_coeff3 = get_coefficients(std_pen_model3, X_pre_train3.columns)
std_pen_model_coeff3

"""L'analisi, a seguito di standardizzazione l1 con alpha=0.001(che produce il miglior f1_score con perceptron), rivela che la feature più irrilevante è *description length*. La più importante è il numero di follwer, seguita dal numero di post e di follows."""

pearson_correlation = X_pre_train3.corr(method='pearson')

fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)

sb.heatmap(pearson_correlation, annot=True)

plt.show()

"""Non si evidenziano rilevanti correlazioni.

Il seguente grafico mostra la distribuzione degli account veri e falsi al variare delle feature più importanti: *#follows*, *#followers* e *#posts*. Tale distribuzione è molto simile a quella studiata precdentemente negli altri dataset.
"""

fig = plt.figure(figsize=(25, 25))
ax = fig.add_subplot(2,2,1,projection='3d')
x = "#follows"
z = "#followers"
y = "#posts"
ax.set_xlabel(x)
ax.set_ylabel(y)
ax.set_zlabel(z)
tmp = dataset3[dataset3["#followers"] < 15000]
account_colors= tmp["fake"].map(account_color_map)
ax.scatter(tmp[x], tmp[y], tmp[z], c=account_colors)

"""## PARTE 3

In questa parte si confrontano i diversi modelli, così come fatto precedentemente.

### Perceptron
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# per_model = Pipeline([
#     ("scaler", StandardScaler()),
#     ("per", Perceptron(n_jobs=-1, random_state=42, class_weight={1:3}))
# ])
# 
# #print(per_model.get_params())
# 
# per_grid = {
#     "scaler": [None, StandardScaler()],
#     "per__penalty": ["l2", "l1", "elasticnet"],
#     "per__alpha": np.logspace(-3, 3, 7),
#     "per__fit_intercept": [False, True]
# }
# 
# per_model3, score = grid_search_with_cross_validation(per_model, per_grid, k_fold, dataset3, "fake", scoring = "f1_macro")
# models["Perceptron3"] = {"Model": per_model, "Score": score}
#

model3 =Pipeline(steps=[('scaler', StandardScaler()),
                ('per',
                 Perceptron(alpha=0.01, class_weight={1: 3}, n_jobs=-1,
                            penalty='l1', random_state=42))])

model3.fit(X_pre_train3, y_pre_train3)
f1_measure = f1_score( y_pre_val3,  model3.predict(X_pre_val3), average="macro")

models["Perceptron3"] = {"Model": model3, "Score": f1_measure}

y_pred3 = model3.predict(X_pre_val3)
measureModel(X_pre_val3, y_pre_val3, y_pred3, model3)

model3.named_steps["per"].coef_

hidden=[3,4]

plot_separator_on_data(X_pre_val3, y_pre_val3, dataset3 ,hidden, model3, "per", repeatCombination = False)

"""Il perceptron ha ottenuto l'f1_measure più basso rispetto al dataset2 e al dataset1.

### Regressione Logistica
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# log_model = Pipeline([
#     ("scaler", StandardScaler()),
#     ("lr", LogisticRegression(solver='liblinear', class_weight={1:3}, random_state=42))
# ])
# 
# #print(log_model.get_params())
# 
# log_grid = {
#     "scaler": [None, StandardScaler()],
#     "lr__penalty": ["l2", "l1"],
#     "lr__C": np.logspace(-4, 2, 7),
#     "lr__fit_intercept": [False, True]
# }
# 
# log_model, score = grid_search_with_cross_validation(log_model, log_grid, k_fold, dataset3,"fake", scoring = "f1_macro")
# models["Logistic Regression3"] = {"Model" : log_model, "Score": score}

log_model3 = Pipeline(steps=[('scaler', None),
                ('lr',
                 LogisticRegression(class_weight={1: 3}, random_state=42,
                                    solver='liblinear'))])

log_model3.fit(X_pre_train3, y_pre_train3)
f1_measure = f1_score( y_pre_val3,  log_model3.predict(X_pre_val3), average="macro")
models["Logistic Regression3"] = {"Model" : log_model3, "Score": f1_measure}

y_pred3 = log_model3.predict(X_pre_val3)
measureModel(X_pre_val3, y_pre_val3, y_pred3, log_model3)

print(log_model3.named_steps['lr'].intercept_)
log_model3.named_steps["lr"].coef_

plot_separator_on_data(X_pre_val3, y_pre_val3, dataset3 ,[], log_model3, "lr", repeatCombination = False)

"""Anche la regressione logistica ha restituito un risultato peggiore rispetto al dataset1 e dataset2.

### Support Vector Machine
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# svm_model = Pipeline([
#     ("scaler", StandardScaler()),
#     ("svc", SVC(random_state=42, class_weight={1:3}))
# ])
# 
# #print(svm_model.get_params())
# 
# svm_grid = [
#   {'svc__C': np.logspace(3, 5, 3), 'svc__kernel': ['linear']},
#   {'svc__C': np.logspace(3, 5, 3), 'svc__gamma': ['scale'], 'svc__kernel': ['rbf']},
# ]
# 
# svm_model, score= grid_search_with_cross_validation(svm_model, svm_grid, k_fold,  dataset3, "fake",  scoring = "f1_macro")
# models["Support Vector Machine3"] = {"Model": svm_model, "Score": score}

svm_model3 =  Pipeline(steps=[('scaler', StandardScaler()),
                ('svc',
                 SVC(C=100000.0, class_weight={1: 3}, kernel='linear',
                     random_state=42))])

svm_model3.fit(X_pre_train3, y_pre_train3)
f1_measure = f1_score( y_pre_val3,  svm_model3.predict(X_pre_val3), average="macro")
models["Support Vector Machine3"] = {"Model": svm_model3, "Score": f1_measure}

y_pred3 = svm_model3.predict(X_pre_val3)
measureModel(X_pre_val3, y_pre_val3, y_pred3, svm_model3)

svm_model3.named_steps["svc"].coef_

plot_separator_on_data(X_pre_val3, y_pre_val3, dataset3 ,[], svm_model3, "svc", lineColor="black", repeatCombination = False)

"""In questo caso, il support vector machine ha restituito un risultato migliore del dataset1 ma comunque peggiore del dataset2.

## PARTE 4
"""

def difference_between_two_models(error1, error2, confidence, y_val):
    z_half_alfa = stats.norm.ppf(confidence)
    variance = (((1 - error1) * error1) / len(y_val)) + (((1 - error2) * error2) / len(y_val))
    d_minus = abs(error1 - error2) - z_half_alfa * (pow(variance, 0.5))
    d_plus = abs(error1 - error2) + z_half_alfa * (pow(variance, 0.5))
    print("Valore minimo: {}\nValore massimo: {}\n".format(d_minus, d_plus))

per_error = 1 - models["Perceptron3"]["Score"]
lre_error = 1 - models["Logistic Regression3"]["Score"]
svm_error = 1 - models["Support Vector Machine3"]["Score"]

print("Support Vector Machine vs Logistic Regression, intervallo di confidenza:")
difference_between_two_models(svm_error, lre_error, 0.95,  y_pre_val3)

print("Support Vector Machine vs Perceptron, intervallo di confidenza:")
difference_between_two_models(svm_error, per_error, 0.95, y_pre_val3)

print("Logistic Regression vs Perceptron, intervallo di confidenza:")
difference_between_two_models( lre_error, per_error, 0.95, y_pre_val3)

"""Support Vector Machine(f1_score=0.890) e Logistic Regression(f1_score=0.899) danno un risultato simile."""

print("F1_measure Perceptron: ", models["Perceptron3"]["Score"])
print("F1_measure Logistic Regression: ", models["Logistic Regression3"]["Score"])
print("F1_measure Support Vector Machine: ", models["Support Vector Machine3"]["Score"])

def confusion_matrix_calculation(model):
    return confusion_matrix(y_pre_val3, model.predict(X_pre_val3))

def confidence(acc, N, Z):
    den = (2*(N+Z**2))
    var = (Z*np.sqrt(Z**2+4*N*acc-4*N*acc**2)) / den
    a = (2*N*acc+Z**2) / den
    inf = a - var
    sup = a + var
    return (inf, sup)

def calculate_accuracy(conf_matrix):
    return np.diag(conf_matrix).sum() / conf_matrix.sum().sum()

conf_perc3 = pd.DataFrame(confusion_matrix_calculation(model3))
conf_log_reg3 = pd.DataFrame(confusion_matrix_calculation(log_model3))
conf_SVM3 = pd.DataFrame(confusion_matrix_calculation(svm_model3))

#con confidenza del 0.95 si ha Z=1.96
pd.DataFrame([confidence(calculate_accuracy(conf_perc3), len(X_pre_val3), 1.96),
              confidence(calculate_accuracy(conf_log_reg3), len(X_pre_val3), 1.96),
              confidence(calculate_accuracy(conf_SVM3), len(X_pre_val3), 1.96)],
                 index=["perceptron", "logreg", "SVM"], columns=["inf", "sup"])

"""Dagli intervalli di confidenza al 95% abbiamo la conferma che Regressione logistica è il modello più accurato."""

hidden = [0,1,2,3,4]

plt.figure(figsize=(70, 70))
plot_separator_on_data(X_pre_val3, y_pre_val3, dataset3 ,hidden, models["Perceptron3"]["Model"], "per", repeatCombination=False, newFig = False, lineColor="green")
plot_separator_on_data(X_pre_val3, y_pre_val3, dataset3 ,hidden, models["Logistic Regression3"]["Model"], "lr", repeatCombination=False, newFig = False, lineColor="violet")
plot_separator_on_data(X_pre_val3, y_pre_val3, dataset3,hidden, models["Support Vector Machine3"]["Model"], "svc", repeatCombination=False, newFig = False, lineColor="black")

"""## PARTE 5

Dopo aver studiato il dataset3, siamo giunti alla conclusione che i risultati non sono troppo buoni. Ciò non ci sorprende: le feature del dataset1 e dataset2 erano abbastanza diverse fra di loro, infatti, molto probabilmente, per il dataset2 sono stati considerati profili di persone molto famose, a differenza del dataset1. Addestrare i modelli partendo quindi da un dataset così disparato non poteva effettuare miglioramenti rispetto ai modelli ottenuti 
rispetto ai singoli dataset.

# Conclusione
"""

nostri_account =  pd.read_csv("dataset_nostri_profili_real.csv")
nostri_account_fake = pd.read_csv("dataset_nostri_profili_fake.csv")

"""Alla luce degli studi dei diversi modelli e dei diversi dataset, è risultato che la predizione migliore si ottiene utilizzando la Regressione Logistica sul nuovo dataset2 (f1_score= 0.945), ovvero il dataset ottenuto eliminando dal dataset2 le istanze con un numero di follower molto alto. Andiamo dunque a testare il Logistic Regression addestrato col dataset2 con i nostri profili Instagram ( e altri falsi cercati da noi), per verificare, con dati mai inseriti, che il modello funzioni."""

nostri_account.rename(columns = {'followersCount':'#followers', 'followsCount':'#follows', 'biography': 'description length',
                           'postsCount': '#posts','profilePicUrl': 'profile pic', 'private':'private',  'username': 'nums/length username',
                           'fullName':'fullname words' , 'externalUrl':'external URL', }, inplace = True)

nostri_account_fake.rename(columns = {'followersCount':'#followers', 'followsCount':'#follows', 'biography': 'description length',
                           'postsCount': '#posts','profilePicUrl': 'profile pic', 'private':'private',  'username': 'nums/length username',
                           'fullName':'fullname words' , 'externalUrl':'external URL', }, inplace = True)

nostri_account["fake"] = 0

nostri_account_fake["fake"] = 1

dataset_nostri_account =  nostri_account.append(nostri_account_fake, ignore_index=True)

dataset_nostri_account = pd.DataFrame(dataset_nostri_account, columns=dataset2.columns)

dataset_nostri_account["name==username"] = np.where( dataset_nostri_account['fullname words'] == dataset_nostri_account["nums/length username"], 1,0)

count = 0
for i in range(len(dataset_nostri_account)):
  count=0;
  if (dataset_nostri_account["nums/length username"][i] is np.NaN):
    dataset_nostri_account["nums/length username"][i] =0
  else:
    for let in dataset_nostri_account["nums/length username"][i]:
      if let.isdigit():
          count+=1;
    dataset_nostri_account["nums/length username"][i] = (float)(count/(len(dataset_nostri_account["nums/length username"][i])));

count = 0
for i in range(len(dataset_nostri_account)):
  count=0;
  print(dataset_nostri_account["fullname words"][i])
  if (dataset_nostri_account["fullname words"][i] is np.NaN):
    dataset_nostri_account["nums/length fullname"][i] =0
  else:
    for let in dataset_nostri_account["fullname words"][i]:
      if let.isdigit():
          count+=1;
    print(count)
    dataset_nostri_account["nums/length fullname"][i] = (float)(count/(len(dataset_nostri_account["fullname words"][i])));

count = 0
for i in range(len(dataset_nostri_account)):
  count=0;
  if (dataset_nostri_account["fullname words"][i] is np.NaN):
    dataset_nostri_account["fullname words"][i] = 0
  else:
    print(dataset_nostri_account["fullname words"][i])
    word_list = dataset_nostri_account["fullname words"][i].split();
    dataset_nostri_account["fullname words"][i] = len(word_list)

def removeNan(feature):
  for i in range(len(dataset_nostri_account)):
    if (dataset_nostri_account[feature][i] is np.NaN):
        dataset_nostri_account[feature][i] = 0
    else:
        dataset_nostri_account[feature][i] = 0

removeNan("profile pic")
removeNan("external URL")

for i in range(len(dataset_nostri_account)):
  if (dataset_nostri_account["private"][i] is False):
      dataset_nostri_account["private"][i] = 0
  else:
      dataset_nostri_account["private"][i] = 1

dataset_nostri_account["description length"] = dataset_nostri_account["description length"].fillna("")

count = 0
for i in range(len(dataset_nostri_account)):
  if (dataset_nostri_account["description length"][i] == ""):
    dataset_nostri_account["description length"][i] = 0
  else:
    dataset_nostri_account["description length"][i] = len(dataset_nostri_account["description length"][i])

dataset_nostri_account["#posts"] = dataset_nostri_account["#posts"].fillna(0)
dataset_nostri_account["#followers"] = dataset_nostri_account["#followers"].fillna(0)
dataset_nostri_account["#follows"] = dataset_nostri_account["#follows"].fillna(0)

dataset_nostri_account

y_to_predict = dataset_nostri_account["fake"]

y_to_predict

X_nostri_account = dataset_nostri_account.drop("fake", axis=1)

"""Di seguito, vengoni messi a confronto le predizioni dei modelli del dataset2 e del nuovo dataset2 e confrontati il vero output.

Modello SVM ottenuto dall'addestramento col nuovo dataset2 (quello che aveva restituito l'f1_measure migliore).
"""

new_svm_y = new_svm_model2.predict(X_nostri_account)
pd.DataFrame(list(zip(y_to_predict, new_svm_y)),
               columns =['Real class', 'Predicted class'])

"""Modello Perceptron ottenuto dall'addestramento col dataset2."""

per_y = model2.predict(X_nostri_account)
pd.DataFrame(list(zip(y_to_predict, per_y)),
               columns =['Real class', 'Predicted class'])

"""Modello Logistic Regression ottenuto dall'addestramento col dataset2."""

log_y = log_model2.predict(X_nostri_account)
pd.DataFrame(list(zip(y_to_predict, log_y)),
               columns =['Real class', 'Predicted class'])

"""Modello SVM ottenuto dall'addestramento col dataset2."""

svm_y = svm_model2.predict(X_nostri_account)
pd.DataFrame(list(zip(y_to_predict, svm_y)),
               columns =['Real class', 'Predicted class'])

"""Modello Perceptron ottenuto dall'addestramento col nuovo dataset2."""

new_per_y = new_per_model2.predict(X_nostri_account)
pd.DataFrame(list(zip(y_to_predict, new_per_y)),
               columns =['Real class', 'Predicted class'])

"""Modello Logistic Regression ottenuto dall'addestramento col nuovo dataset2."""

new_log_y = new_log_model2.predict(X_nostri_account)
pd.DataFrame(list(zip(y_to_predict, new_log_y)),
               columns =['Real class', 'Predicted class'])

hidden =[0,2,3,4,5,6,7,9,10]

"""Il seguente grafico rappresenta le tre rette generate dai tre modelli del nuovo dataset2, sovrapposte al nostro dataset. """

yff =dataset_nostri_account[dataset_nostri_account["#followers"] < 2000000]["fake"]
xff = dataset_nostri_account[dataset_nostri_account["#followers"] < 2000000].drop(["fake"], axis=1)

plt.figure(figsize=(150, 150))
plot_separator_on_data(xff, yff, dataset_nostri_account, hidden, new_svm_model2, 
                       "svc", lineColor="green", newFig= False, repeatCombination=False)
plot_separator_on_data(xff, yff, dataset_nostri_account, hidden, new_per_model2,
                       "per", lineColor="violet", newFig= False, repeatCombination=False)
plot_separator_on_data(xff, yff, dataset_nostri_account, hidden, new_log_model2,
                       "lr", lineColor="black", newFig= False, repeatCombination=False)

hidden =[0,1,2,3,4,5,6,7,8]

"""Il seguente grafico rappresenta le tre rette generate dai tre modelli del dataset2, sovrapposte al nostro dataset. """

yff =dataset_nostri_account[dataset_nostri_account["#followers"] < 20000]["fake"]
xff = dataset_nostri_account[dataset_nostri_account["#followers"] < 20000].drop(["fake"], axis=1)

plt.figure(figsize=(150, 150))
plot_separator_on_data(xff, yff, dataset_nostri_account, hidden, svm_model2, 
                       "svc", lineColor="green", newFig= False, repeatCombination=False)
plot_separator_on_data(xff, yff, dataset_nostri_account, hidden, model2,
                       "per", lineColor="violet", newFig= False, repeatCombination=False)
plot_separator_on_data(xff, yff, dataset_nostri_account, hidden, log_model2,
                       "lr", lineColor="black", newFig= False, repeatCombination=False)

"""La seguente funzione realizza un grafico tridimensionale e il relativo piano di separazione delle classi.

"""

fig = plt.figure(figsize=(20, 20))
ax = fig.gca(projection='3d')

tmp = dataset_nostri_account
xi = 5
yi = 8
zi = 10
x = tmp.columns[xi]
y = tmp.columns[yi]
z = tmp.columns[zi]

fake =  "fake"
ax.set_xlabel(x)
ax.set_ylabel(y)
ax.set_zlabel(z)

account_colors= tmp[fake].map(account_color_map)
# Plot Curve Fit
# Create dataframe with of x_fit and y_fit

#print(tmp)

# Pass to the model's predict() method to return z-fit


#print(zz)

#ax.set_zlim(0,10000)

ax.scatter(tmp[x], tmp[y], tmp[z], c=account_colors)
xlim, ylim = plt.xlim(), plt.ylim()

X = np.arange(0, tmp[x].max(), tmp[x].max()/2)
Y = np.arange(0,tmp[y].max(), tmp[y].max()/2)
X, Y = np.meshgrid(X, Y)
Z = separator_3d(svm_model2, X, Y,"svc",xi,yi,zi)

Z = np.asarray(Z).reshape(-1, 2)

surf = ax.plot_surface(X,Y,Z, linewidth=0, antialiased=False, alpha = 0.2)
plt.show()

